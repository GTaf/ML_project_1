{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from implementations_JB import *\n",
    "from proj1_helpers import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, x, ids =  load_csv_data(\"train.csv\", sub_sample=True)\n",
    "#y_test, x_test, ids_test = load_csv_data(\"test.csv\")\n",
    "data_list = full_data_processing(y,x,ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below contains a function to dislay the histogram of a choosen feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histo(index_feature, y, x,bins):\n",
    "    \n",
    "    x_array = x[:,index_feature]\n",
    "    x_mask = np.where(yb>0, True, False) # Transforming in a boolean mask\n",
    "    \n",
    "    print np.mean(x_array)\n",
    "    print np.std(x_array)\n",
    "    \n",
    "    # Displaying the histogram for the whole data\n",
    "    plt.hist(x_array,bins)\n",
    "    plt.ylabel(\"Feature \"+str(i))\n",
    "    \n",
    "    # Displaying the histogram for the data where y = 1\n",
    "    plt.figure()\n",
    "    plt.hist(x_array[x_mask],bins)\n",
    "    plt.ylabel(\"Feature \"+str(i))\n",
    "    \n",
    "    # Displaying the histogram for the data where y = -1\n",
    "    plt.figure()\n",
    "    plt.hist(x_array[np.invert(x_mask)],bins)\n",
    "    plt.ylabel(\"Feature \"+str(i))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below contains functions to split the data according to :\n",
    "    - The value of PRI_jet_num (0,1,2 or 3)\n",
    "    - The value of DER_Mass_MMC (-999 or well-defined value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitting1(y,x,ids):\n",
    "    for i in range(4):\n",
    "        mask = (x[:,22] == i)\n",
    "        yield y[mask], x[mask], ids[mask]\n",
    "        \n",
    "def splitting2(y,x,ids):\n",
    "    feature0 = x[:,0]\n",
    "    masks = [0,0]\n",
    "    mask1 = np.where(feature0==-999,False,True)\n",
    "    masks[0] = mask1\n",
    "    masks[1] = np.invert(mask1)\n",
    "    for mask in masks:\n",
    "        yield y[mask], x[mask], ids[mask]\n",
    "        \n",
    "def full_splitting(y,x,ids):\n",
    "    \"\"\"\n",
    "    Will yields in this order :\n",
    "    split_id = 0) Data with a defined feature0 ans with PRI_jet_num = 0\n",
    "    split_id = 1) Data with a defined feature0 ans with PRI_jet_num = 1\n",
    "    split_id = 2) Data with a defined feature0 ans with PRI_jet_num = 2\n",
    "    split_id = 3) Data with a defined feature0 ans with PRI_jet_num = 3\n",
    "    split_id = 4) Data with an undefined feature0 ans with PRI_jet_num = 0\n",
    "    split_id = 5) Data with an undefined feature0 ans with PRI_jet_num = 1\n",
    "    split_id = 6) Data with an undefined feature0 ans with PRI_jet_num = 2\n",
    "    split_id = 7) Data with an undefined feature0 ans with PRI_jet_num = 3\n",
    "    \"\"\"\n",
    "    split_id = -1\n",
    "    for y2,x2,ids2 in splitting2(y,x,ids):\n",
    "        for y1,x1,ids1 in splitting1(y2,x2,ids2):\n",
    "            split_id+=1\n",
    "            yield y1,x1,ids1, split_id           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below contains functions to prepare the data :\n",
    "    - Remove some unrelevant features if needed\n",
    "    - Transform the angle features in two features by applying sin() and cos()\n",
    "    - Applying the absolute value to symetric features\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_features_jetnum(x,split_id):\n",
    "    \n",
    "    \"\"\"\n",
    "    Remove the useless features if the jet num feature is equal to 0 (--> split_id equal to 0 or 4)\n",
    "    Remove the useless features if the jet num feature is equal to 1 (--> split_id equal to 1 or 5)\n",
    "    \"\"\"\n",
    "    useless_features_index = []\n",
    "    if split_id in [0,4]:\n",
    "        useless_features_index = [4, 5, 6, 12,22, 23, 24, 25 , 26, 27, 28,29,33,34] \n",
    "    elif split_id in [1,5]:\n",
    "        useless_features_index = [4, 5, 6, 12,22, 26, 27, 28,34]\n",
    "    else:\n",
    "        return x\n",
    "    mask = np.ones(int(x.shape[1]), dtype=bool)\n",
    "    mask[(useless_features_index)] = False\n",
    "    return x[:,mask]\n",
    "\n",
    "def angle_processing(x):\n",
    "    \n",
    "    angle_features = [15, 18, 20, 25, 28]\n",
    "    new_x = np.zeros((x.shape[0], x.shape[1] + len(angle_features) ))\n",
    "    \n",
    "    for k in range(x.shape[1]):\n",
    "        if k not in angle_features:\n",
    "            new_x[:, k] = x[:, k]\n",
    "        \n",
    "    for idx, column in enumerate(angle_features): \n",
    "        \n",
    "        new_x[:, column] = np.cos(x[:, column])\n",
    "        new_x[:, x.shape[1] + idx] = np.sin(x[:, column])\n",
    "    \n",
    "    return new_x\n",
    "\n",
    "def absolute_value(x,split_id):\n",
    "    \n",
    "    list0 = [10,11,13,14,16,18,19,20]\n",
    "    list1 = [10,11,13,14,16,19,20,22,23,24,25]\n",
    "    list2 = [14,15,17,18,20,24,25,27,28,30,31,32,33,34]\n",
    "    list3 = [14,15,17,18,20,24,25,27,28,30,31,32,33,34]\n",
    "\n",
    "    full_list = [list0, list1, list2, list3]\n",
    "    \n",
    "    list_ = full_list[split_id%4]\n",
    "\n",
    "    for index in np.array(list_):\n",
    "        x[:,index] = np.absolute(x[:,index])\n",
    "    return x    \n",
    "\n",
    "def full_data_processing(y,x,ids):\n",
    "    data_list = []\n",
    "    for y0,x0,ids0, split_id in full_splitting(y,x,ids):\n",
    "        \n",
    "        x0 = angle_processing(x0)\n",
    "        x0 = remove_features_jetnum(x0,split_id)\n",
    "        x0 = standardize(x0)\n",
    "        x0 = absolute_value(x0,split_id)\n",
    "    \n",
    "        data_list.append([y0,x0,ids0])\n",
    "        \n",
    "    return data_list  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below contains functions to do the regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression(y_test, x_test, y_train, x_train,lambda_):\n",
    "    \n",
    "    # Here we can choose which method to use :\n",
    "    \n",
    "    #w, train_loss = ridge_regression(y_train, x_train, lambda_)   \n",
    "    #w, train_loss = least_squares_GD_adapt_step(y_train, x_train, 0*np.random.rand(int(x_train.shape[1])), 100, 0.00001,computeLoss = True);\n",
    "    #w, train_loss = lasso_GD_adapt_step(y_train, x_train, 0*np.random.rand(int(x_train.shape[1])), 200, 0.000001, lambda_, computeLoss = True)\n",
    "    w, train_loss = logistic_regression(y_train, x_train, 0*np.random.rand(int(x_train.shape[1])), 100, 0.00001)\n",
    "    #w, train_loss = reg_logistic_regression(y, tx, lambda_,initial_w, max_iter, gamma)\n",
    "    \n",
    "    test_loss = compute_loss(y_test, x_test, w)\n",
    "    y_pred = predict_labels(w, x_test)\n",
    "    \n",
    "    return w, train_loss, test_loss, y_pred \n",
    "\n",
    "def build_poly(x, degree):\n",
    "    \"\"\"polynomial basis functions for input data x, for j=0 up to j=degree.\"\"\"\n",
    "    poly = np.ones((len(x), 1))\n",
    "    for deg in range(1, degree+1):\n",
    "        poly = np.c_[poly, np.power(x, deg)]\n",
    "    return poly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below contains functions to train the model using K-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)\n",
    "\n",
    "def K_split(y,x,k_indices):\n",
    "    \n",
    "    for indices_list in k_indices:\n",
    "        mask = np.ones(y.shape[0],dtype=bool)\n",
    "        mask[indices_list] = False\n",
    "        \n",
    "        yield y[mask], standardize(x[mask]), y[np.invert(mask)], standardize(x[np.invert(mask)])\n",
    "\n",
    "\n",
    "def Kfold_regression(y, x, k,degree,lambda_):\n",
    "    \n",
    "    seed = 1\n",
    "    k_indices = build_k_indices(y, k, seed)\n",
    "    \n",
    "    train_loss_list = []\n",
    "    test_loss_list = []\n",
    "    score_list = []\n",
    "    \n",
    "    for y_train, x_train, y_test, x_test, in K_split(y,x,k_indices):\n",
    "        x_train_poly = build_poly(x_train,degree)\n",
    "        x_test_poly = build_poly(x_test,degree)\n",
    "        w, train_loss, test_loss, y_pred = regression(y_test, x_test_poly, y_train, x_train_poly,lambda_)\n",
    "        train_loss_list+=[train_loss]\n",
    "        test_loss_list+=[test_loss]\n",
    "        score_list+=[np.mean(y_pred == y_test)]\n",
    "        \n",
    "    return train_loss_list, test_loss_list,score_list  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below contain a function to test the 2 parameters : the degree and lambda_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def param_test_simultane(y, x, k):\n",
    "    \n",
    "    lambda_list = np.logspace(-8, -1, 15)\n",
    "    degree_list = np.arange(3,15)\n",
    "    \n",
    "    final_train_loss_list = np.zeros((len(degree_list),len(lambda_list)))\n",
    "    final_test_loss_list = np.zeros((len(degree_list),len(lambda_list)))\n",
    "    final_score_list = np.zeros((len(degree_list),len(lambda_list)))\n",
    "\n",
    "    \n",
    "    \n",
    "    for i, lambda_ in enumerate(lambda_list):\n",
    "        for j,degree in enumerate(degree_list):\n",
    "            \n",
    "            train_loss_list, test_loss_list,score_list = Kfold_regression(y, x,k,degree,lambda_)\n",
    "            train_loss, test_loss, score = np.mean(train_loss_list), np.mean(test_loss_list), np.mean(score_list)\n",
    "            final_train_loss_list[j,i]=train_loss\n",
    "            final_test_loss_list[j,i]=test_loss\n",
    "            final_score_list[j,i]=score\n",
    "            \n",
    "            print (\"Degree = \" + str(degree) + \" and Lambda = \" + str(lambda_) + \", Score = \" + str(score))\n",
    "    \n",
    "        \"\"\"\n",
    "        plt.figure()\n",
    "        plt.plot(degree_list,final_train_loss_list[:,i])\n",
    "        plt.ylabel(\"Train Loss\")\n",
    "        plt.figure()\n",
    "        plt.plot(degree_list,final_test_loss_list[:,i])\n",
    "        plt.ylabel(\"Test Loss\")\n",
    "        plt.figure()\n",
    "        plt.plot(degree_list,final_score_list[:,i], label = str(lambda_))\n",
    "        plt.ylabel(\"Score for lambda = \" + str(lambda_))\n",
    "        plt.ylim(0.6, 1)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \"\"\"\n",
    "\n",
    "    plt.figure()\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(10,10)\n",
    "    for i, lambda_ in enumerate(lambda_list):\n",
    "        plt.plot(degree_list, final_score_list[:,i], label = str(lambda_))\n",
    "        plt.ylabel(\"Score\")\n",
    "\n",
    "    plt.ylim(np.min(final_score_list), np.max(final_score_list))\n",
    "    plt.legend()\n",
    "    plt.savefig(\"test\" + str(i) + \".png\")\n",
    "    plt.show()\n",
    "    \n",
    "    return final_train_loss_list, final_test_loss_list, final_score_list\n",
    "\n",
    "for i in range(8):\n",
    "    param_test_simultane(data_list[i][0], data_list[i][1],6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below contain a function to make a csv file to upload it on Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submission(deg_list, lambda_list,y, x, ids, y_test, x_test, ids_test):\n",
    "    \n",
    "    data_list_train = full_data_processing(y,x,ids)\n",
    "    data_list_test = full_data_processing(y_test,x_test,ids_test)\n",
    "    \n",
    "    ids_final = np.array([])\n",
    "    y_pred_final = np.array([])\n",
    "\n",
    "    for i in range(len(data_list_train)):\n",
    "        degree = deg_list[i]\n",
    "        lambda_ = lambda_list[i]\n",
    "        x_poly = build_poly(data_list_train[i][1], degree)\n",
    "        x_poly_test = build_poly(data_list_test[i][1], degree)\n",
    "        \n",
    "        w, train_loss, test_loss, y_pred = regression(data_list_test[i][0], x_poly_test, data_list_train[i][0], x_poly, lambda_)\n",
    "    \n",
    "        ids_final = np.append(ids_final, data_list_test[i][2])\n",
    "        y_pred_final = np.append(y_pred_final, y_pred)\n",
    "        \n",
    "    create_csv_submission(ids_final, y_pred_final, \"prediction.csv\")        \n",
    "\n",
    "deg_list = [12,12,12,12,8,4,5,4]\n",
    "lambda_list = [1e-05,1e-05,0.001,0.0001,1e-05,1e-05,0.0001,0.001]\n",
    "#deg_list = [12,12,12,13,5,5,5,3]\n",
    "#lambda_list = [3.1622776601683795e-05,0.001,1e-6,0.001,1e-5,1e-6,0.0001,3.1622776601683795e-05]\n",
    "submission(deg_list,lambda_list,y, x, ids, y_test, x_test, ids_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-822.07255614]\n",
      "(64L, 1186L)\n",
      "S\n",
      "B\n",
      "H\n",
      "822.0725561440946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "implementations_JB.py:164: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-t))\n",
      "implementations_JB.py:168: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = y.T.dot(np.log(pred)) + (1 - y).T.dot(np.log(1 - pred))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan nan nan ... nan nan nan]\n",
      "(64L, 1186L)\n",
      "S\n",
      "B\n",
      "H\n",
      "[nan nan nan ... nan nan nan]\n"
     ]
    },
    {
     "ename": "LinAlgError",
     "evalue": "Singular matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-00de0042f760>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mparam_test_simultane\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-9-58b8f80be3b0>\u001b[0m in \u001b[0;36mparam_test_simultane\u001b[1;34m(y, x, k)\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdegree\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdegree_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m             \u001b[0mtrain_loss_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loss_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscore_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKfold_regression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdegree\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlambda_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m             \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loss_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_loss_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m             \u001b[0mfinal_train_loss_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-a2ebc89bf4c1>\u001b[0m in \u001b[0;36mKfold_regression\u001b[1;34m(y, x, k, degree, lambda_)\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mx_train_poly\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_poly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdegree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mx_test_poly\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_poly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdegree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mregression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test_poly\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_train_poly\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlambda_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m         \u001b[0mtrain_loss_list\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mtest_loss_list\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_loss\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-dcffd845b805>\u001b[0m in \u001b[0;36mregression\u001b[1;34m(y_test, x_test, y_train, x_train, lambda_)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m#w, train_loss = least_squares_GD_adapt_step(y_train, x_train, 0*np.random.rand(int(x_train.shape[1])), 100, 0.00001,computeLoss = True);\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m#w, train_loss = lasso_GD_adapt_step(y_train, x_train, 0*np.random.rand(int(x_train.shape[1])), 200, 0.000001, lambda_, computeLoss = True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogistic_regression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.00001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[1;31m#w, train_loss = reg_logistic_regression(y, tx, lambda_,initial_w, max_iter, gamma)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Dell\\Documents\\ML_Projects\\ML_project_1\\implementations_JB.py\u001b[0m in \u001b[0;36mlogistic_regression\u001b[1;34m(y, tx, initial_w, max_iter, gamma_)\u001b[0m\n\u001b[0;32m    216\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0miter\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m         \u001b[1;31m# get loss and update w.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 218\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlearning_by_newton_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    219\u001b[0m         \u001b[1;31m# converge criterion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Dell\\Documents\\ML_Projects\\ML_project_1\\implementations_JB.py\u001b[0m in \u001b[0;36mlearning_by_newton_method\u001b[1;34m(y, tx, w)\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhessian\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogistic_regression_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m     \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 202\u001b[1;33m     \u001b[0mw\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhessian\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    203\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Programmes\\Anaconda\\lib\\site-packages\\numpy\\linalg\\linalg.pyc\u001b[0m in \u001b[0;36minv\u001b[1;34m(a)\u001b[0m\n\u001b[0;32m    526\u001b[0m     \u001b[0msignature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'D->D'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misComplexType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m'd->d'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m     \u001b[0mextobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_linalg_error_extobj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_raise_linalgerror_singular\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 528\u001b[1;33m     \u001b[0mainv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_umath_linalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mextobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    529\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mainv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Programmes\\Anaconda\\lib\\site-packages\\numpy\\linalg\\linalg.pyc\u001b[0m in \u001b[0;36m_raise_linalgerror_singular\u001b[1;34m(err, flag)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_raise_linalgerror_singular\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLinAlgError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Singular matrix\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_raise_linalgerror_nonposdef\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLinAlgError\u001b[0m: Singular matrix"
     ]
    }
   ],
   "source": [
    "param_test_simultane(data_list[0][0], data_list[0][1],6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
